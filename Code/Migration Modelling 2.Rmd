---
title: "Migration Model"
output: html_document
date: "2023-01-27"
---

Upload Data
```{r}
library(readr)
urlfile <- 'https://raw.githubusercontent.com/Celv03/Final-Year-Project/main/Migration%20Full%20Data%20-%20Clear%20Names.csv'
MgrDt_Github <- read_csv(url(urlfile)) #Upload from Github
MgrDt_Github
MgrDt
```

```{r}
MgrDt <- read.csv(file = "/Users/kelvin/Desktop/Maths/Final Year Project/Migration Full Data - Clear Names.csv", blank.lines.skip = FALSE)
colnames(MgrDt)
```

Scale our GDP wrt to Population

```{r}
MgrDt$GDP.A <- MgrDt$GDP.A/MgrDt$Popultion.A
MgrDt$GDP.O <- MgrDt$GDP.O/MgrDt$Popultion.O
MgrDt$GDP.D <- MgrDt$GDP.O/MgrDt$GDP.A

```

Filter out Non-European Countries

```{r}
EuroDt <- MgrDt[MgrDt$Continent.O == 'Europe',] # Fix to looking at European Asylums moving out 
EuroDt <- EuroDt[!EuroDt$Country.O == 'Unknown',] #Remove asylums of unkown country as we have entire voids we cannot fill 
```

Shortern UK names and name our unknowns Also remove any columns that bring no info

```{r}
EuroDt <- Filter(function(x)(length(unique(x))>1), EuroDt) # Removes single valued factors for our columns No use for us as it won't mean much & take space 
names(EuroDt)
dim(EuroDt)
EuroDt$Country.O[(EuroDt$Country.O == '')] = 'UKN' # change all the blank unknown iso's to UKN
EuroDt$Country.A[EuroDt$Country.A == ''] = 'UKN'
EuroDt$Country.O[(EuroDt$Country.O == 'United Kingdom of Great Britain and Northern Ireland')] = 'United Kingdom' # change all the blank unknown iso's to UKN
EuroDt$Country.A[EuroDt$Country.A == 'United Kingdom of Great Britain and Northern Ireland'] = 'United Kingdom'

```

Asign row names to our data and then split data into numberical vs cateogrial. Our categorical data is complete so we do not need to perform analysis

```{r}
Orgn.to.Asy.Iso<- paste(EuroDt$Year, EuroDt$ISO.O, 'to' , EuroDt$ISO.A) #Get our abr asylum moves
row.names(EuroDt) <- Orgn.to.Asy.Iso
EuroDt.Char <- Filter(function(x)(typeof(x) == 'character'), EuroDt) # keep the ones w/ characters stored
EuroDt.Num <- Filter(function(x)(typeof(x) != 'character'), EuroDt) # Filter out any columns which use characters i.e non-numerical characters
Orgn.to.Asy.Iso <- row.names(EuroDt)

```

Box Plots for Outliers scale & produce our first box plot

```{r}
EuroOrigin <- EuroDt
par(mfrow=c(1,1))
par(mar=c(10,4,2,1))
EuroDt.Num2 <- scale(EuroDt.Num, center = FALSE, scale = TRUE)
boxplot(EuroDt.Num[,2:17], las=2, main = 'Box plots of Origin Country Features')
boxplot(EuroDt.Num2[,c(2:17)], las=2, main = 'Scaled Box plots of Origin Country Features') 
abline(h=0)

```

Identify outlying countries

```{r}
features <- c('X..1.90.O', 'Popultion.O', 'Maternal.Mortality.O', 'Homicide.O')
scaler <- attr(EuroDt.Num2, 'scaled:scale')[features] 
summary(EuroDt.Num2[,features])
iqr <- c()
for (i in 1:4){ iqr<-c(iqr,IQR(EuroDt.Num2[,features[i]], na.rm = TRUE)) } 
unique(EuroOrigin[EuroOrigin$X..1.90.O > 4*scaler[1],2]) # Moldova
unique(EuroOrigin[EuroOrigin$Popultion.O > 2*scaler[2],2]) # Russia
unique(EuroOrigin[-(EuroOrigin$Popultion.O < 0.002*scaler[2]) | #Under 108K Pop - SMR AND GIB MCO LIE
                    (EuroOrigin$Popultion.O > 2*scaler[2]) | #Over 100M Pop - RUS
                    is.na(EuroOrigin$Popultion.O), 2]) #No Value - VAT
unique(EuroOrigin[EuroOrigin$Maternal.Mortality.O > 2.6*scaler[3],2]) # RUS, ROU
unique(EuroOrigin[EuroOrigin$Homicide.O > 2.6*scaler[4],2]) # RUS

```

Remove outlying countries and then plot our boxplots again

```{r}
ctry.to.remove <- c('RUS', 'AND', 'GIB', 'MCO', 'SMR', 'LIE', 'VAT', 'MDA')
EuroDt <- EuroOrigin[!(EuroOrigin$ISO.O %in% ctry.to.remove),]
filter <- rownames(EuroDt)
EuroDt.Char <- EuroDt.Char[filter,]
EuroDt.Num <- EuroDt.Num[filter,]
colours <- cbind(EuroDt.Num2[,c(2:17)], 1)
colours[!(rownames(colours) %in% filter),17] <- 2
boxplot(EuroDt.Num2[,c(2:17)], las=2,
        main = 'Scaled Box plots of Origin Country Features') 
abline(h=0)
boxplot(scale(EuroDt[,4:19], center = FALSE), las=2
        , main = 'Filtered & Scaled Box plots of Origin Country Features') 
abline(h=0)
par(mar=c(5.1, 4.1, 4.1, 2.1))

```

1 Missing Data analysis define our function that finds our principal components with m dimensions then transforms it back to our data original data

```{r}
fit.svd <- function(X, M = 1) { # mat mult subset size M of d * v^t this finds us our zij
  svdob <- svd(X)
  with(svdob, 
       u[, 1:M, drop = FALSE] %*% (d[1:M] * t(v[,1:M, drop = FALSE] ) )
  ) }
```

Scale our work and create global averages

```{r}
Glob.Scl.EuroDt <- scale(EuroDt.Num, center = TRUE, scale = TRUE)  # Create a global scale 
glob_xbar <- colMeans(Glob.Scl.EuroDt, na.rm = TRUE) # Create a full column mean in case our local ones are just NA 
glob_xhat <- EuroDt.Num # glob_xhat will be our end result

```

Code for iterating through and filling missing values with PCA

```{r}
for (country in unique(EuroDt$ISO.O)) {  # Look into each country and do a Missing data analysis 
  filter <- rownames(EuroDt[EuroDt$ISO.O==country,])
  Scl.EuroDt <- scale(EuroDt.Num[filter,], center = TRUE, scale = TRUE)  # Scale (1) # This is our data-set with missing data
  dt.nas <- Scl.EuroDt # our data with "holes"
  index.na <- is.na(dt.nas) # find them and rec their location 
  inb <- which(index.na == TRUE, arr.ind= TRUE)[,2] # which column they're in
  ina <- which(index.na == TRUE, arr.ind= TRUE)[,1] # which row they're in
  Xhat <- dt.nas # what will be our new data 
  xbar <- colMeans(dt.nas, na.rm = TRUE) # calc the mean of our columns
  nul_cols <- names(xbar[is.na(xbar)])
  xbar[is.na(xbar)] <- glob_xbar[is.na(xbar)]
  # Fix Scaling
  attr(Xhat, 'scaled:scale')[nul_cols] <- attr(Glob.Scl.EuroDt, 'scaled:scale')[nul_cols]
  attr(Xhat, 'scaled:center')[nul_cols] <- attr(Glob.Scl.EuroDt, 'scaled:center')[nul_cols]
  inb.na <- which(is.na(xbar), arr.ind = TRUE)
  Xhat[index.na] <- xbar[inb] # fill the empty vals with col averages (2)
  sum(is.na(Xhat)) # null check
  thresh <- 1e-7 # create our error margin (difference in old mse vs new mse)
  rel_err <- 1 # initial 
  iter <- 0
  ismiss <- is.na(dt.nas) # Finding the row & column of our missing data
  mssold <- mean((scale(dt.nas, xbar, FALSE)[!ismiss])^2)  #store mse of non missing vals 
  mss0 <- mean(dt.nas[!ismiss]^2) # meansq of non missing vals
  while (rel_err > thresh) { # (5)
    iter <- iter + 1
    Xapp <- fit.svd(Xhat, M = 3) #comp our Princ Comp (3)
    Xhat[ismiss] <- Xapp[ismiss] #replace the mean xbars with our new princ comp (4)
    mss <- mean(((dt.nas - Xapp)[!ismiss])^2)
    rel_err <- (mssold - mss)/mss0 # old mse - new mse/ variance 
    mssold <- mss
    #cat('Iter:', iter, 'MSS:', mss, 'Rel. Err:', rel_err, '\n')
  }
  Xhat
  unscl.Xhat <- t(apply(Xhat, 1, 
          function(r) r * attr(Xhat, 'scaled:scale') + 
            attr(Xhat, 'scaled:center')))
  cat(country, ' ', sum(is.na(unscl.Xhat)), '\n' )
  glob_xhat[filter,] <- unscl.Xhat # Check for blanks 
}

```

2.1 Serbia Example Identify rows we want to find for our example

```{r}
excols <- c(1:18)
example <- rownames(EuroDt[EuroDt$ISO.O=='SRB',])
dt.nas2 <- Glob.Scl.EuroDt
index.na2 <- is.na(dt.nas2)
inb2 <- which(index.na2 == TRUE, arr.ind= TRUE)[,2]
xbar <- colMeans(dt.nas2, na.rm = TRUE)
dt.nas2[index.na2] <- xbar[inb2]

```

Find when we add local means, global means and when hard impute is used

```{r}
means <- t(apply(dt.nas2, 1, 
                 function(r) r * attr(dt.nas2, 'scaled:scale') + 
                   attr(dt.nas2, 'scaled:center')))[, excols]
xbar.srb <- colMeans(dt.nas2[example,], na.rm = TRUE)
dt.nas2[index.na2] <- xbar.srb[inb2]
means.loc <- t(apply(dt.nas2, 1, 
                 function(r) r * attr(dt.nas2, 'scaled:scale') + 
                   attr(dt.nas2, 'scaled:center')))[, excols]
iter12 <- glob_xhat[, excols]
Initial <- EuroDt[example,excols]
```

Average our points for Hard impute

```{r}
points <- iter12[example,c(1,16)]
Average <- data.frame(matrix(NA, ncol = 2))
colnames(Average) <- c('Year','Education.Budget.O')
for (Year in unique(points[,1])){
  Education.Budget.O <- mean(points[points[,1] == Year, 2])
  Average <- rbind(Average, data.frame(Year, Education.Budget.O))
}
```

Plot our graph for comparison

```{r}
par(mfrow = c(1,1))
plot(Initial[example,c(1,18)], type = 'l', main = "Serbia's % Education Budget per Year")
points(means[example,c(1,16)] , col = 2)
lines(means[example,c(1,16)], type = 'l' , col = 2, lty = 2 )
points(means.loc[example,c(1,16)] , col = 4)
lines(means.loc[example,c(1,16)], type = 'l' , col = 4, lty = 2 )
points(Average, col = 3)
lines(Average, type = 'l' , col = 3, lty = 2 )
legend("bottomleft", inset=.02, title="Dataset",
       c("Original","European Mean","Serbia Mean","Matrix Completion"),
       fill=c("black","red","Blue","Green"), horiz=FALSE, cex=1)

```

Summarise our data to just origin countries

```{r}
Orgn.to.Asy.Iso <- rownames(glob_xhat)
EuroDt.no_na <- glob_xhat
df.Eurodt <- data.frame(EuroDt.Char, EuroDt.no_na, row.names = Orgn.to.Asy.Iso, stringsAsFactors=FALSE)
Ctry.Dt <- data.frame(df.Eurodt[2], df.Eurodt[6:22], df.Eurodt[56], row.names = NULL, stringsAsFactors=FALSE)
col.nam <- c(names(Ctry.Dt[2:18]), 'Avg.Pop.p100k' )
euro.cntry <- unique(df.Eurodt$ISO.O) # split our countries 
n <- length(unique(df.Eurodt$ISO.O))
v <- data.frame(matrix(NA, ncol = 18))
colnames(v) <- col.nam
for (Country in euro.cntry){
  cnrty.in <- Ctry.Dt[Ctry.Dt$ISO.O == Country,]
  # data[row, col]
  Avg.Pop.p100k <- sum(cnrty.in[19])/nrow(unique(cnrty.in[2]))
  v <- rbind(v, data.frame(t(colMeans(cnrty.in[2:18])), Avg.Pop.p100k) ) #want 100k per decision to be sum of countries but mean of years
}
v <- v[2:(n+1),] # remove our NA's
row.names(v) <- euro.cntry
Orgn.co.dt <- v
```

Summarise our data to just origin countries & years

```{r}
Ctry.Dt <- data.frame(df.Eurodt[2], df.Eurodt[6:22], df.Eurodt[56], row.names = NULL, stringsAsFactors=FALSE)
euro.cntry <- unique(paste(df.Eurodt$ISO.O, df.Eurodt$Year)) # split our countries 
n <- length(euro.cntry)
w <- data.frame(matrix(NA, ncol = 18))
col.nam <- c(names(Ctry.Dt[2:18]), 'Pop.p100k' )
colnames(w) <- col.nam
for (i in 1:n){
  Country <- substr(euro.cntry[i], 1,3)
  Year <- substr(euro.cntry[i], 5,8)
  cnrty.in <- Ctry.Dt[Ctry.Dt$ISO.O == Country 
                      & Ctry.Dt$Year == Year,]
  #Pop.p100k <- sum(cnrty.in[18])/nrow(unique(cnrty.in[2]))
  Pop.p100k <- sum(cnrty.in[,'Total'])
  w <- rbind(w, data.frame(t(colMeans(cnrty.in[2:18])), Pop.p100k, stringsAsFactors = FALSE) ) #want 100k per decision to be sum of countries but mean of years
}
w <- w[2:(n+1),] # remove our NA's
row.names(w) <- euro.cntry
Cnty_data <- w
df.Eurodt # full version EuroDt.no_na is just numerics
Cnty_data
```

Sum for Country, Year, & Continent
```{r}
Cont.dt <- data.frame(df.Eurodt[c(2,5:39,56)], row.names = NULL, stringsAsFactors=FALSE)
euro.Cont <- unique(paste(df.Eurodt$ISO.O, df.Eurodt$Year, df.Eurodt$Continent.A)) # split our countries 
n <- length(euro.Cont)
w <- data.frame(matrix(NA, ncol = 35))
col.nam <- c(names(Cont.dt[3:36]), 'Pop.p100k' )
colnames(w) <- col.nam
for (i in 1:n){
  Country <- substr(euro.Cont[i], 1,3)
  Year <- substr(euro.Cont[i], 5,8)
  Continent.A <- substr(euro.Cont[i], 10,20)
  cnrty.in <- Cont.dt[Cont.dt$ISO.O == Country 
                      & Cont.dt$Year == Year
                      & Cont.dt$Continent.A == Continent.A,]
  #Pop.p100k <- sum(cnrty.in[18])/nrow(unique(cnrty.in[2]))
  Pop.p100k <- sum(cnrty.in[,'Total'])
  w <- rbind(w, data.frame(t(colMeans(cnrty.in[3:36])), Pop.p100k, stringsAsFactors = FALSE) ) #want 100k per decision to be sum of countries but mean of years
}
w$Continent.A <- substr(euro.Cont, 10,20)
w <- w[2:(n+1),] # remove our NA's
row.names(w) <- euro.Cont
conti_data <- w
conti_data
```

```{r}
write.csv(Orgn.co.dt, "/Users/kelvin/Desktop/Maths/Final Year Project/OriginCountrySummary.csv")
```

```{r}
write.csv(Cnty_data, "/Users/kelvin/Desktop/Maths/Final Year Project/AnnualCountryMigration.csv")
```



PCA Example

```{r}
Orgn.co.dt <- v 
Cnty_data[,which(is.na(Cnty_data), arr.ind = TRUE)[,2]]
pr.out <- prcomp(Orgn.co.dt, scale = TRUE)  # scale = True will auto normalise our variables for this analysis to N(0, 1)
pr.out$center
pr.out$scale 
pr.out$rotation # Returns our ϕ1, ϕ2, ... 
dim(pr.out$x) # x is our xij's X has dimensions 50 x 4
biplot(pr.out, scale = 0) #PCA only differs from a sign flip'
PCAdt <- pr.out$x[,1:2]
plot(PCAdt, main = '1st and 2nd Principal Components Plot')
pr.var <- pr.out$sdev^2
pve <- pr.var/sum(pr.var) #calculate our explained variance
round(cumsum(pve), digits = 2)
plot(cumsum(pve), xlab = 'Princ Comp', ylab = 'Cummla Prop of Var Expla', ylim = c(0, 1), type = 'b')
```

K-means Clustering

```{r}
PCAdt <- pr.out$x[,1:2]
plot(PCAdt, main = '1st & 2nd Principal Components')
text(PCAdt[,1], PCAdt[,2], row.names(PCAdt), cex=0.6, pos=4, col="red")
rownames <- c('Death.Rate.O', 'X..Rural.O', 'GDP.O', 'Growth.O'
              , 'Birth.Rate.O', 'Popultion.O')
g <- pr.out$rotation[rownames, 1:2]
len <- 6
arrows(0, 0, len * g[, 1],  len * g[, 2], length = 0.02, col = "darkcyan" )
text(len * g[,1], len * g[,2], row.names(g), cex=0.6, pos=4, col = "darkcyan")
```

```{r}
plot(Cnty_data[c(2:4,18)])
plot(Cnty_data[c(5:7,18)])
plot(Cnty_data[c(8:10,18)])
plot(Cnty_data[c(11:13,18)])
plot(Cnty_data[c(14:16,18)])
plot(Cnty_data[c(17,18)])
```

```{r}

Cnty_data2 <- Cnty_data[Cnty_data$Year %in% c(2001:2005),]
Mig <- data.frame(Edu = Cnty_data2$GDP.O, Pop = Cnty_data2$Popultion.O)
row.names(Mig) <- row.names(Cnty_data2)
plot(Mig)
test <- kmeans(Mig,9)
plot(Mig, col = (test$cluster + 1)) 
points(test$centers, pch = 4, cex = 3)

```

K-means when k = 2, 6, 11

```{r}
km.out <- kmeans(PCAdt, 2, nstart = 5) 
km.out$centers
par(mfrow = c(1, 1))
plot(PCAdt, col = (km.out$cluster + 1), main = 'K-means Cluster results w/ k = 2', xlab = '', ylab = '')
text(PCAdt[,1], PCAdt[,2], row.names(PCAdt), cex=0.6, pos=4, col=(km.out$cluster + 1))
points(km.out$centers, col = (km.out$cluster + 4), pch = 4, cex = 3)
km.out$tot.withinss

km.out <- kmeans(PCAdt, 6, nstart = 20)
plot(PCAdt, col = (km.out$cluster + 1), main = 'K-means Cluster results w/ k = 6', xlab = 'PC1', ylab = 'PC2') 
text(PCAdt[,1], PCAdt[,2], row.names(PCAdt), cex=0.6, pos=4, col=(km.out$cluster + 1))
points(km.out$centers, col = (km.out$cluster + 9), pch = 4, cex = 3)
km.out$tot.withinss

km.out11 <- kmeans(PCAdt, 11, nstart = 20)
plot(PCAdt, col = (km.out11$cluster + 1), main = 'K-means Cluster results w/ k = 11', xlab = 'PC1', ylab = 'PC2') 
text(PCAdt[,1], PCAdt[,2], row.names(PCAdt), cex=0.6, pos=4, col=(km.out11$cluster + 1))
points(km.out11$centers, col = (km.out11$cluster + 9), pch = 4, cex = 3)
km.out11$tot.withinss

```

hierarchial Clustering complete vs average vs single

```{r}
x <- Orgn.co.dt
hc.complete <- hclust(dist(scale(x)), method = 'complete')
hc.average <- hclust(dist(scale(x)), method = 'average')
hc.single <- hclust(dist(scale(x)), method = 'single')
plot(hc.complete, main = " Complete Linkage ", xlab = "", sub = "", cex = .9)
abline(h = 7.5, col = 'red')
plot(hc.average , main = " Average Linkage ", xlab = "", sub = "", cex = .9)
plot(hc.single, main = " Single Linkage ", xlab = "", sub = "", cex = .9)
```

```{r}
hc.complete <- hclust(dist(scale(x)), method = 'complete')
n <- attributes(dist(x))$Size # Number of variables
data.m <- as.matrix(dist(x)^2) # Type conversion for model fitting
ori.sse <- sum ( dist(x)^2 )/n # RSS
# Initial rule : improvement of RSS
alpha <- 0.01*ori.sse
gcv <- c()
rss <- c()
i <- 1
improv1 <- sum(dist(x)^2)/n 
while ((i <= n) & ( improv1 > alpha )) {
  memb <- cutree(hc.complete , k=i)
  # Calculate total RSS for each cluster structure
  if (i == 1)
    rss[i] <- sum ( dist(x)^2 )/n
  else {
    rss[i] <- 0
  for (j in 1:i) {
    clus <- data.m[ which ( memb ==j) ,which ( memb ==j)]
    rss[i] <- rss [i] + sum( clus ) / (2* sum ( memb ==j))
  }
  }
# GCV
trA <- i
gcv[i] <- n * rss [i] / (n-trA) ^2
improv1 <- rss[i]
i = i+1
}
```

```{r}
plot(gcv, type = 'o', xlab = 'No. of Clusters', ylab = 'Generalised Cross Validation', main = 'Cluster number vs GCV Statistic', col = 3)

```

using complete hierarchial clustering with n = 12 find mean and standard deviation on all groups then decide which group to use for modelling

```{r}
n <- 5
(sort(Country_Grp <- cutree(hc.complete,n))) # will look at 2,3,7
#(sort(Country_Grp <- sort(km.out11$cluster)) )# will look at 2,3,7
analysis <- c()
for (i in unique(Country_Grp)){
  (Grp1 <- names(Country_Grp[Country_Grp == i]))
  Grp10dt <- df.Eurodt[df.Eurodt$ISO.O %in% Grp1,]
  Grp1_Ctry <- unique(Grp10dt$Country.O)
  Grp_nm <- paste('Group ', i, ':', toString(Grp1_Ctry))
  train <- sample(1:nrow(Grp10dt), nrow(Grp10dt)/2)
  Grp10dt.test <- Grp10dt[-train, "Total"]
  cat(Grp_nm, '\n', 'sd:', sd(Grp10dt.test), '\n', 'mean', mean(Grp10dt.test),'\n')
#  for (j in Grp1_Ctry){
#  cat(j, mean(Grp10dt[-(train) & (Grp10dt$Country.O == j),'Total']),'\n') 
#    }
}
```

```{r}
i <- 3
(Grp1 <- names(Country_Grp[Country_Grp == i]))
Grp10dt <- df.Eurodt[df.Eurodt$ISO.O %in% Grp1,]
#Grp10dt <- Grp10dt[Grp10dt$Year > 2004,]
#Grp10dt$ISO.O <-c(CHE=1,AUT=2, SRB=3, BGR = 4)[Grp10dt$ISO.O]
#Grp10dt$ISO.A <- 1:length(unique(Grp10dt$ISO.A))
Country_Code <- cbind(1:length(unique(Grp10dt$ISO.A)), unique(Grp10dt$ISO.A))
colnames(Country_Code) <- c('No','Country')
grpex <- Filter(function(x)(typeof(x) != 'character'), Grp10dt)
grpex <- Filter(function(x)(length(unique(x))>1), grpex)
#biplot(pr.out)
pr.out <- prcomp(grpex, scale = TRUE) 
PCAdt <- pr.out$x[,1:2]
plot(PCAdt)
text(PCAdt[,1], PCAdt[,2], row.names(PCAdt), cex=0.6, pos=4, col="red")
pr.var <- pr.out$sdev^2
pve <- pr.var/sum(pr.var) #calculate our explained variance
round(cumsum(pve), digits = 2)

```

further "tidy up some points that don't seem grouped together

```{r}
hm.out <- hclust(dist(grpex), method = 'complete')
km.out <- kmeans(grpex, 2, nstart = 5) 
par(mfrow = c(1, 1))
plot(PCAdt, col = (km.out$cluster + 1), main = 'K-means Cluster results w/ k = 2', xlab = '', ylab = '')
# 2 Red, 3 Green, 4 Blue
choice <- which(table(km.out$cluster) == max(table(km.out$cluster)))
main_cluster <- names(km.out$cluster[km.out$cluster == choice])
Grp10dt <- Grp10dt[main_cluster,-c(3,4)]
Grp1_Ctry <- unique(Grp10dt$Country.O)
(Grp_nm <- paste('Group:', toString(Grp1_Ctry)))
```

Create a sample of training data and create our set of testing data

Scale my response to N(0,1) then compare trends and then model with trees etc. Then Unscale result. Do Residual analysis to check if which models are more suitable.


```{r}
train <- sample(1:nrow(Grp10dt), nrow(Grp10dt)/2)
Grp10dt.test <- Grp10dt[-train, "Total"]
names(Grp10dt.test) <- rownames(Grp10dt[-train,])
```

Train our models: linear

```{r}
flmdl <- lm(Total~ log(Popultion.O) + . - Country.O , Grp10dt, subset = train)
asy.lm <- step(flmdl, direction = "backward", k = log(n))
summary(asy.lm)
asy.lm$coefficients
```

Splines

```{r}
library('splines')
spflmdl <- lm(Total~ bs(Growth.O + Popultion.O + GDP.O + X..1.90.O 
                        + Malnourished.O + Maternal.Mortality.O 
                        + Suicide.Mortality.O + Life.Expectancy.O 
                        + X..Male.O + X..Rural.O + Death.Rate.O 
                        + Birth.Rate.O + Natural.Increase.O 
                        + X..Crop.Production.O + Education.Budget.O 
                        + Homicide.O), Grp10dt, subset = train)
asy.spl <- step(spflmdl, direction = "backward", k = log(n))
summary(asy.spl)
```

Trees & Pruning

```{r}
library('tree')
asy.tree <- tree(Total~ log(Popultion.A) + . , Grp10dt, subset = train)
summary(Grp10dt$Total)
summary(asy.tree)
library('rpart')
library('rpart.plot')
asy.tree.r <- rpart(Total~ log(Popultion.A) + ., Grp10dt, subset = train)
rpart.plot(asy.tree.r)
plot(asy.tree)
text(asy.tree, pretty = 0)
# Check if  our tree is worth pruning
cv.asy <- cv.tree(asy.tree)
plot(cv.asy$size, cv.asy$dev, type = 'b') # our dev is our deviance & size is the no. of branches
prune.asy <- prune.tree(asy.tree, best = 4) # looks like it's worth pruning, past 5-7ish our deviance doesn't go down much
plot(prune.asy, main = Grp_nm)
text(prune.asy, pretty = 0)
summary(prune.asy)
```

Random forest

```{r}
library(randomForest)
Bag.Tree <- randomForest(Total ~ log(Popultion.O) + ., Grp10dt
                         , subset = train, mtry = 20, ntree = 1000,  importance = TRUE)
Bag.Tree
```

```{r}
library(gbm)
boost.tree <- gbm(Total~.,data=Grp10dt[train,-(1:5)], distribution="gaussian"
                  , n.trees =5000, interaction.depth = 3, shrinkage = 0.2)
boost.tree
summary(boost.tree)
```

# Lin model

```{r}
yhat.lm <- predict(asy.lm, newdata = Grp10dt[-train,])
names(yhat.lm) <- rownames(Grp10dt[-train,])
plot(yhat.lm, Grp10dt.test, main = Grp_nm )
abline(0,1)
mse.lm <- mean((yhat.lm - Grp10dt.test)^2)
mse.lm^0.5
```

#spline

```{r}
yhat.spl <- predict(asy.spl, newdata = Grp10dt[-train,])
plot(yhat.spl, Grp10dt.test, main = Grp_nm )
abline(0,1)
mse.spl <- mean((yhat.spl - Grp10dt.test)^2)
mse.spl^0.5
```

# Tree

```{r}
yhat <- predict(asy.tree, newdata = Grp10dt[-train,])
plot(yhat, Grp10dt.test, main = Grp_nm )
abline(0,1)
mse <- mean((yhat - Grp10dt.test)^2)
mse^0.5
```

# Prune

```{r}
yhat <- predict(prune.asy, newdata = Grp10dt[-train,])
plot(yhat, Grp10dt.test, main = Grp_nm )
abline(0,1)
prune.mse <- mean((yhat - Grp10dt.test)^2)
prune.mse^0.5
```

#Bagging/Random forest Tree

```{r}
yhat <- predict(Bag.Tree, newdata = Grp10dt[-train,])
plot(yhat, Grp10dt.test, main = Grp_nm )
abline(0,1)
bag.mse <- mean((yhat - Grp10dt.test)^2)
bag.mse^0.5
```

#Boosting

```{r}
yhat <- predict(boost.tree, newdata = Grp10dt[-train,], n.trees = 1000)
names(yhat) <- rownames(Grp10dt[-train,])
plot(yhat, Grp10dt.test, main = Grp_nm )
abline(0,1)
boost.mse <- mean((yhat - Grp10dt.test)^2)
boost.mse^0.5
```

Results comparison

```{r}
results <- c(sd(Grp10dt.test), mse.lm^0.5, mse.spl^0.5, mse^0.5, prune.mse^0.5, bag.mse^0.5, boost.mse^0.5, mean(Grp10dt.test))
model_names <- c('sd', 'lm','spline','Tree','Pruning','Bagg','Boost','Mean')
names(results) <- model_names
barplot(results, main = Grp_nm, xlab = 'Model Name', ylab = 'Mean Square Error')
cat(' Sd:', sd(Grp10dt.test),  ' vs LM:', mse.lm^0.5, ' vs Spl:', mse.spl^0.5, '\n'
    ,'Tree: ', mse^0.5, ' vs Prune:', prune.mse^0.5, ' vs Bagging:', bag.mse^0.5, '\n'
    , 'Boosting:', boost.mse^0.5, ' & Mean:', mean(Grp10dt.test), '\n', Grp_nm, '\n')

```

Important Variables Test:

```{r}
varImpPlot(Bag.Tree, type = 1, main = 'Features & Impact on MSE of Migration Model'
           , sub='For: Hungary, Romania, Croatia, Bulgaria, Belarus & Serbia',
           col.main="darkred", col.lab="blue", col.sub="black")
```

We see here that Natural increase of the asylum country is one of the main factors in the rise of migration. With death rates of the asylum countries or difference being equally high. The first 5 variables is fairly high in colleration though

Linear vs Trees vs Random Forest vs Boosting: alot of code jist is to summarise each country and to compare the outwards migration of 1 country instead of where it goes

```{r}
yhat.lm <- predict(asy.lm, newdata = Grp10dt[-train,])
yhat.bag <- predict(Bag.Tree, newdata = Grp10dt[-train,])
yhat.tree <- predict(asy.tree, newdata = Grp10dt[-train,])
yhat.bst <- predict(boost.tree, newdata = Grp10dt[-train,])

yhat.lm <- unscaler(yhat.lm)
yhat.bag <- unscaler(yhat.bag)
yhat.tree <- unscaler(yhat.tree)
names(yhat.bst) <- rownames(Grp10dt[-train,])
yhat.bst <- unscaler(yhat.bst)

Test.Summ <- unique(substr(names(Grp10dt.test), 1, 8))
Test.Actuals <- rep(NA, length(Test.Summ))
names(Test.Actuals) <- Test.Summ
Test.Pred.tree <- rep(NA, length(Test.Summ))
names(Test.Pred.tree) <- Test.Summ
Test.Pred.bag <- rep(NA, length(Test.Summ))
names(Test.Pred.bag) <- Test.Summ
Test.Pred.lm <- rep(NA, length(Test.Summ))
names(Test.Pred.lm) <- Test.Summ
Test.Pred.bst <- rep(NA, length(Test.Summ))
names(Test.Pred.bst) <- Test.Summ
for (i in Test.Summ) {
  Test.Actuals[i] <- sum(Grp10dt.test[which(substr(names(Grp10dt.test), 1, 8) == i )])
  Test.Pred.tree[i] <- sum(yhat.tree[which(substr(names(yhat.tree), 1, 8) == i )])
  Test.Pred.bag[i] <- sum(yhat.bag[which(substr(names(yhat.bag), 1, 8) == i )])
  Test.Pred.lm[i] <- sum(yhat.lm[which(substr(names(yhat.lm), 1, 8) == i )])
  Test.Pred.bst[i] <- sum(yhat.bst[which(substr(names(yhat.lm), 1, 8) == i )])
}

# (Outlying <- which((Test.Actuals<200) | (Test.Actuals>5000), arr.ind = TRUE))
# Models <- c(mean((Test.Actuals[-Outlying] - Test.Pred.lm[-Outlying])^2)^0.5,
# mean((Test.Actuals[-Outlying] - Test.Pred.tree[-Outlying])^2)^0.5,
# mean((Test.Actuals[-Outlying] - Test.Pred.bag[-Outlying])^2)^0.5,
# mean((Test.Actuals[-Outlying] - Test.Pred.bst[-Outlying])^2)^0.5,
# mean(Test.Actuals[-Outlying]),
# sd(Test.Actuals[-Outlying]))
# names(Models) <- c('LM','Tree','RandomForest','Boosting', 'Mean', 'Standard Dev')
Models <- c(mean((Test.Actuals - Test.Pred.lm)^2)^0.5,
mean((Test.Actuals - Test.Pred.tree)^2)^0.5,
mean((Test.Actuals - Test.Pred.bag)^2)^0.5,
mean((Test.Actuals - Test.Pred.bst)^2)^0.5,
mean(Test.Actuals),
sd(Test.Actuals))
names(Models) <- c('LM','Tree','RandomForest','Boosting', 'Mean', 'Standard Dev')

Models
```

plot them

```{r}
plot((Grp10dt.test[-Outlying]), (yhat.tree[-Outlying]), xlab = 'Test Values'
     , ylab = 'Model Values', main = 'Model vs Test:  Hungary, Romania, Croatia, Bulgaria, Belarus & Serbia'
     , sub = 'Country to Country per Year', col.main="darkred", col.lab="blue", col.sub="black")
points(Grp10dt.test,yhat.bag, col = 4)
points(Grp10dt.test,yhat.lm, col = 'darkred')
#points(Grp10dt.test,yhat.bst, col = 'darkgreen')
abline(0,1)
legend('topleft', inset=.02, title="Model",
       c("Tree","Random Forest","Linear Model", 'Boosting'),
       fill=c("black","blue", "darkred", 'darkgreen'), horiz=FALSE, cex=.9)

plot((Test.Actuals[-Outlying]), (Test.Pred.tree[-Outlying]), xlab = 'Test Values'
     , ylab = 'Model Values', main = 'Model vs Test:  Hungary, Romania, Croatia, Bulgaria, Belarus & Serbia'
     , sub = 'Country per Year', col.main="darkred", col.lab="blue", col.sub="black")
points((Test.Actuals),(Test.Pred.bag), col = 4)
points((Test.Actuals),(Test.Pred.lm), col = 'darkred')
#points((Test.Actuals),(Test.Pred.bst), col = 'darkgreen')
abline(0,1)
legend('topleft', inset=.02, title="Model",
       c("Tree","Random Forest","Linear Model"),
       fill=c("black","blue", "darkred"), horiz=FALSE, cex=.6)

```

Need to do this for IBM Task: create 3-4 Boosting models - Need: Country Group, MSE, 3 biggest impacts, migration per country

```{r}
library(randomForest)
df.Impact <- data.frame(Country = c(), KeyIndicators=c())
df.Results <- data.frame(Country = c(), Migration = c())
df.Actuals <- data.frame(Year = c(), Country = c(), Migration = c())
df.RMSE <- data.frame(Country = c(), RMSE=c())
for (i in 1:4){
  Grp1 <- names(Country_Grp[Country_Grp == i])
  Grp10dt <- df.Eurodt[df.Eurodt$ISO.O %in% Grp1,]
  # Grp10dt.save <- Grp10dt 
  # Grp10dt <- Grp10dt[(Grp10dt$Total > quantile(Grp10dt$Total, c(.06))) & (Grp10dt$Total < quantile(Grp10dt$Total, c(.94))), ]
  #Grp10dt <- df.Eurodt[Cnty_data$ISO.O %in% Grp1,]
  #Grp10dt <- Grp10dt[Grp10dt$Year > 2004,]
  grpex <- Filter(function(x)(typeof(x) != 'character'), Grp10dt)
  grpex <- Filter(function(x)(length(unique(x))>1), grpex)
  train <- (sample(1:nrow(Grp10dt), nrow(Grp10dt)/2) | (Grp10dt$Year == 2021))
  Bag.Tree <- randomForest(Total ~ log(Popultion.O) + ., Grp10dt
                           , subset = train, mtry = 20, ntree = 1000,  importance = TRUE)
  Importance <- names(sort(round(Bag.Tree$importance[,1]), TRUE)[1:3])
  yhat.bag <- predict(Bag.Tree, newdata = Grp10dt[(Grp10dt$Year == 2021),])
  bag.rmse <- mean((yhat - Grp10dt[(Grp10dt$Year == 2021),'Total'])^2)^0.5
  Test.Actuals.21 <- c()
  Test.Actuals <- c()
  Test.Pred.bag.21 <- c()
  Grp10dt.test <- Grp10dt[Grp10dt$Year == 2021,'Total']
  names(Grp10dt.test) <- row.names(Grp10dt[Grp10dt$Year == 2021,])
  Test.Summ.21 <- unique(substr(rownames(Grp10dt[-train,]), 6, 8))
  for (i in Test.Summ.21) { 
    Test.Actuals.21[i] <- sum(Grp10dt.test[which(substr(names(Grp10dt.test), 6, 8) == i )])
    Test.Pred.bag.21[i] <- sum(yhat.bag[which(substr(names(yhat.bag), 6, 8) == i )])
  }
  Grp10dt.test <- Grp10dt[Grp10dt$Year %in% c(2015:2021),'Total']
  names(Grp10dt.test) <- row.names(Grp10dt[Grp10dt$Year %in% c(2015:2021),])
  Test.Summ <- unique(substr(rownames(Grp10dt[Grp10dt$Year %in% c(2015:2021),]), 1, 8))
  for (i in Test.Summ) { 
    Test.Actuals[i] <- sum(Grp10dt.test[which(substr(names(Grp10dt.test), 1, 8) == i )])
  }
  for (i in names(Test.Pred.bag.21)){
    df.Impact <- rbind(df.Impact, cbind(Country = i,KeyIndicators = Importance))
  }
  df.RMSE <- rbind(df.RMSE, data.frame(Country=names(Test.Pred.bag.21) ,RMSE = bag.rmse))
  df.Results <- rbind(df.Results, cbind(Country = names(Test.Pred.bag.21), Migration = Test.Pred.bag.21))
  df.Actuals <- rbind(df.Actuals, data.frame(Year = as.numeric(substr(names(Test.Actuals), 1, 4)), Country = substr(names(Test.Actuals), 6, 8), Migration = Test.Actuals))
}
# write.csv(df.Results,"/Users/kelvin/Desktop/Maths/Final Year Project/CognosResult.csv", row.names=FALSE)
# write.csv(df.Actuals,"/Users/kelvin/Desktop/Maths/Final Year Project/CognosActuals.csv", row.names=FALSE)
# write.csv(df.RMSE,"/Users/kelvin/Desktop/Maths/Final Year Project/CognosRMSE.csv", row.names=FALSE)
# write.csv(df.Impact,"/Users/kelvin/Desktop/Maths/Final Year Project/CognosImpact.csv", row.names=FALSE)
df.Results
df.Actuals
df.RMSE
df.Impact
```
